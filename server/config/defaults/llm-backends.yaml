# LLM Backends â€” each needs service_class (dotted import path), config, system_message
llm_backends:
  openclaw:
    description: "OpenClaw gateway (local)"
    service_class: "backends.openclaw.OpenClawLLMService"
    config:
      gateway_url: "ws://localhost:18789"
    system_message: "You are a helpful assistant."
  moltis:
    description: "Moltis LLM"
    service_class: "backends.moltis.MoltisLLMService"
    config: {}
    system_message: "You are a helpful AI assistant."
  pi:
    description: "Pi personal AI"
    service_class: "backends.pi.PiLLMService"
    config: {}
    system_message: "You are Pi, a helpful and friendly personal AI."
